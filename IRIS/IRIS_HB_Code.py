# -*- coding: utf-8 -*-
"""IRIS_HB.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AVafTUQjcLYVDbpgl-cD23x7CKOFG8xH
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install pykan
# %pip install matplotlib
# %pip install torch
# %pip install numpy
# %pip install pandas

# Commented out IPython magic to ensure Python compatibility.
# %pip install -U scikit-learn
# %pip install tqdm

from kan import KAN
import matplotlib.pyplot
import torch
import numpy
import pandas
from sklearn.preprocessing import MinMaxScaler

# Get program to use GPU
device = "cuda:0" if torch.cuda.is_available() else "cpu"

print(device)

processes = ['HB', 'SM']

data_NN = {} # define empty dictionary to hold dataframes that will be used to train the NN

data_NN = {"HB": {},
          "SM": {}}

NN_inputs = [
             'DER_mass_MMC',
             'DER_mass_transverse_met_lep',
             'DER_mass_vis',
             'DER_pt_h',
             'DER_deltaeta_jet_jet',
             'DER_mass_jet_jet',
             'DER_prodeta_jet_jet',
             'DER_deltar_tau_lep',
             'DER_pt_tot',
             'DER_sum_pt',
             'DER_pt_ratio_lep_tau',
             'DER_met_phi_centrality',
             'DER_lep_eta_centrality',
             'PRI_tau_pt',
             'PRI_tau_eta',
             'PRI_tau_phi',
             'PRI_lep_pt',
             'PRI_lep_eta',
             'PRI_lep_phi',
             'PRI_met',
             'PRI_met_phi',
             'PRI_met_sumet',
             'PRI_jet_num',
             'PRI_jet_leading_pt',
             'PRI_jet_leading_eta',
             'PRI_jet_leading_phi',
             'PRI_jet_subleading_pt',
             'PRI_jet_subleading_eta',
             'PRI_jet_subleading_phi',
             'PRI_jet_all_pt',
             'Weight',
             'NN_output'
            ] # list of variables for Neural Network

!pip install gdown
!gdown https://drive.google.com/uc?id=1adGzuUl0kWrlo6HDTOct5Q10IqTVmMfx
eventsList = pandas.read_csv("HIGGS.csv").values.tolist()
print(eventsList[0])

#Create the datastructure for the dictionary
for process in processes:
    for input in NN_inputs:
        data_NN[process][input] = []

#Fill the dictionary with appropriate values
for process in processes:
    for i in range(len(eventsList)):
        for j in range(0, len(NN_inputs) - 1):
            if process == "HB" and eventsList[i][32] == "s":
                data_NN[process][NN_inputs[j]].append(eventsList[i][j+1])
            elif process == "SM" and eventsList[i][32] == "b":
                data_NN[process][NN_inputs[j]].append(eventsList[i][j+1])

# Convert data into single list
sample_events = []
test_events = []
discovery_events = []

#Add training data
for i in eventsList:
  if i[33] == 't':
    sample_events.append(i)

#Add training data
for i in eventsList:
  if i[33] == 'b':
    test_events.append(i)

#Add training data
for i in eventsList:
  if i[33] == 'v':
    discovery_events.append(i)

sample_events_copy = sample_events.copy()
test_events_copy = test_events.copy()
discovery_events_copy = discovery_events.copy()

#Create Label lists
sample_HB = []
test_HB = []
discovery_HB = []

#Set label lists and get rid of labels as well as two other irrelevant features
for i in sample_events:
  sample_HB.append(i[32])
  i.pop(0)
  i.pop(32)
  i.pop(33)
  i.pop(34)

for i in test_events:
  test_HB.append(i[32])
  i.pop(0)
  i.pop(32)
  i.pop(33)
  i.pop(34)

for i in discovery_events:
  discovery_HB.append(i[32])
  i.pop(0)
  i.pop(32)
  i.pop(33)
  i.pop(34)

# Create and manipulate data (70% train, 30% test)
train_input = sample_events
train_label = sample_HB
test_input = test_events
test_label = test_HB
discover_input = discovery_events
discovery_label = discovery_HB

# Convert data to PyTorch tensors and assign to correct device
train_input = torch.tensor(train_input, dtype=torch.float64).to(device)
train_label = torch.tensor(
    train_label, dtype=torch.double).unsqueeze(1).to(device)
test_input = torch.tensor(test_input, dtype=torch.float64).to(device)
test_label = torch.tensor(
    test_label, dtype=torch.double).unsqueeze(1).to(device)
discover_input = torch.tensor(discover_input, dtype=torch.float64).to(device)
discovery_label = torch.tensor(
    discovery_label, dtype=torch.double).unsqueeze(1).to(device)

# Initialize model, specify `double` if the model expects double precision
model = KAN(width=[29, 30, 30, 1], grid=4, k=4, device=device).double()
'''
MODIFY THE WIDTH AS YOU WISH TO TRY GET AS HIGH AN ACCURACY AS POSSIBLE
'''

def train_acc():
    return (model(train_input).argmax(dim=1) == train_label).float().mean()


def test_acc():
    print((model(test_input).argmax(dim=1) == test_label).float().mean())
    return (model(test_input).argmax(dim=1) == test_label).float().mean()

# Run training with adjusted metric calculations
results = model.train({
    'train_input': train_input,
    'train_label': train_label,
    'test_input': test_input,
    'test_label': test_label
}, opt="LBFGS", steps=17, metrics=(train_acc, test_acc), loss_fn=torch.nn.MSELoss(), device=device)

'''
MODIFY THE STEPS AS YOU WISH TO TRY GET AS LOW A LOSS AS POSSIBLE
'''

def correction(x):
    if x <0.5:
        return 0
    else:
        return 1

def accuracy(model, x, y):
    result = model(x)
    correct = 0
    for i in range(len(result)):
        #print(f"Result: {correction(result[i])}, Actual: {y[i]}")
        if correction(result[i]) == y[i]:
            correct += 1
    print(correct/len(x))

y_predicted_train = model(discover_input[0:len(train_input)]).cpu()
y_predicted_test = model(discover_input[0:len(test_input)]).cpu()
y_predicted_discover = model(discover_input[0:len(discover_input)]).cpu()

accuracy(model, train_input, train_label)

'''
USE HERE TO TEST ACCURACY ON TRAIN AND TEST AND IF TIME ALLOWS ON DISCOVERY
AND THEN SEND THEM TO ME
'''

cumulative_events = 0 # start counter for total number of events for which output is saved
for s in processes: # loop over samples
    data_NN[s]['NN_output'] = y_predicted_discover[cumulative_events:cumulative_events+len(data_NN[s])]
    cumulative_events += len(data_NN[s]) # increment counter for total number of events

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

def calculate_z_scores(dnn_outputs, mean_background, std_dev_background):
    """Calculates Z-scores for DNN outputs given background mean and std dev."""
    z_scores = (dnn_outputs - mean_background) / std_dev_background
    return z_scores

def plot_z_score_distribution(z_scores):
    """Plots the distribution of Z-scores and estimates significance."""
    plt.hist(z_scores, bins=30, density=True, alpha=0.6, label='Z-score Distribution')
    plt.xlabel('Z-score')
    plt.ylabel('Density')
    plt.title('Distribution of Z-scores')

    # Fit a normal distribution to the Z-scores
    mu, std = norm.fit(z_scores)
    xmin, xmax = plt.xlim()
    x = np.linspace(xmin, xmax, 100)
    p = norm.pdf(x, mu, std)
    plt.plot(x, p, 'k', linewidth=2, label='Normal Fit')
    plt.legend()

    # Estimate significance (assuming 5 sigma threshold for discovery)
    p_value = 1 - norm.cdf(5, mu, std)  # Probability of Z > 5
    significance = -norm.ppf(p_value)   # Convert p-value to sigma
    print(f"Estimated Significance from method 1: {significance:.2f} sigma")

    plt.show()

from statistics import stdev

# Example usage (replace with your actual DNN outputs)
dnn_outputs_discover = y_predicted_discover  # Simulated DNN outputs between 0 and 1

dnn_outputs_discover_list = []

for i in dnn_outputs_discover:
    dnn_outputs_discover_list.append(i.detach().numpy())


dnn_discover_background_outputs_list = []

for i in range(len(discovery_events_copy)):
    if discovery_events_copy[i][32] == 'b':
        dnn_discover_background_outputs_list.append(dnn_outputs_discover_list[i])

# Background only events to test H0
mean_background = np.average(dnn_discover_background_outputs_list)  # Mean DNN output for background events
std_dev_background = stdev(dnn_discover_background_outputs_list)  # Standard deviation of DNN output for background events

z_scores = calculate_z_scores(np.array(dnn_outputs_discover_list), mean_background, std_dev_background)
plot_z_score_distribution(z_scores)

'''
SEND ME THIS PLOT AND THE NUMBER BELOW THE GRAPH
'''

import math

classified_signal = []
classified_background = []

for i in range(len(dnn_outputs_discover_list)):
  if dnn_outputs_discover_list[i] <0.5:
    classified_signal.append(discovery_events_copy[i][31])
  else:
    classified_background.append(discovery_events_copy[i][31])

signal_weight = sum(classified_signal)
background_weight = sum(classified_background)

AMS = math.sqrt(2 * ((signal_weight + background_weight + 10) * math.log(1 + signal_weight/(background_weight + 10)) - signal_weight))

print(f"Estimated Significance from method 2: {AMS:.2f} sigma")

'''
SEND ME THIS NUMBER
'''

import numpy as np
import matplotlib.pyplot as plt

dnn_outputs_train = y_predicted_train
dnn_outputs_test = y_predicted_test

dnn_outputs_train_list = []
dnn_outputs_test_list = []

dnn_train_background_outputs_list = []
dnn_train_signal_outputs_list = []
dnn_test_background_outputs_list = []
dnn_test_signal_outputs_list = []


for i in range(len(sample_events_copy)):
    if sample_events_copy[i][32] == 'b':
        dnn_train_background_outputs_list.append(dnn_outputs_train_list[i])
    else:
        dnn_train_signal_outputs_list.append(dnn_outputs_test_list[i])

for i in range(len(test_events_copy)):
    if test_events_copy[i][32] == 'b':
        dnn_test_background_outputs_list.append(dnn_outputs_train_list[i])
    else:
        dnn_test_signal_outputs_list.append(dnn_outputs_test_list[i])

# Data for plot
np.random.seed(42)  # For reproducibility
signal_train_prob = dnn_train_signal_outputs_list
bkg_train_prob = dnn_train_background_outputs_list
signal_test_prob = dnn_test_signal_outputs_list
bkg_test_prob = dnn_test_background_outputs_list

# Calculate binned probabilities and uncertainties
bins = np.linspace(0, 1, 21)  # Define bins for the histogram
signal_train_y, _, _ = plt.hist(signal_train_prob, bins=bins, alpha=0, density=True)
bkg_train_y, _, _ = plt.hist(bkg_train_prob, bins=bins, alpha=0, density=True)

# Calculate error bars using binomial proportion confidence interval
def binom_conf_interval(p, n, z=1.96): # 95% confidence interval
    se = np.sqrt(p * (1 - p) / n)
    return z * se

signal_test_y, _, _ = plt.hist(signal_test_prob, bins=bins, alpha=0, density=True)
signal_test_yerr = binom_conf_interval(signal_test_y, len(signal_test_prob))
bkg_test_y, _, _ = plt.hist(bkg_test_prob, bins=bins, alpha=0, density=True)
bkg_test_yerr = binom_conf_interval(bkg_test_y, len(bkg_test_prob))

# Plotting
plt.figure(figsize=(10, 6))

# Histograms for training data
plt.bar(bins[:-1], signal_train_y, width=np.diff(bins), align="edge", alpha=0.7, color='red', label='S (train)')
plt.bar(bins[:-1], bkg_train_y, width=np.diff(bins), align="edge", alpha=0.7, color='blue', label='B (train)')

# Scatter plot with error bars for test data
plt.errorbar(
    (bins[:-1] + bins[1:]) / 2, signal_test_y,
    yerr=signal_test_yerr, fmt='o', color='purple', label='S (test)'
)
plt.errorbar(
    (bins[:-1] + bins[1:]) / 2, bkg_test_y,
    yerr=bkg_test_yerr, fmt='o', color='blue', label='B (test)'
)

# Formatting
plt.yscale('log')
plt.xlabel('Classifier Output (Probability of being signal)')
plt.ylabel('Density')
plt.title('Classifier Output for Signal and Background')
plt.legend()
plt.grid(alpha=0.4)

plt.show()


'''
SEND ME THIS PLOT
'''

